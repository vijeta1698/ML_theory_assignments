{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. What is prior probability? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A priori probability, also known as classical probability, is a probability that is deduced from formal reasoning. In other words, a priori probability is derived from logically examining an event. A priori probability does not vary from person to person (as would a subjective probability) and is an objective probability.\n",
    "\n",
    "p = f/n\n",
    "\n",
    "Where:\n",
    "\n",
    "f refers to the number of desirable outcomes.\n",
    "N refers to the total number of outcomes.\n",
    "\n",
    "A priori probability requires formal reasoning. For example, consider a coin toss. What is the a priori probability of a head in a single coin toss?\n",
    "\n",
    "One can argue that given a coin has two sides, both of which have equal surface areas, that it is symmetrical. Ignoring the possibility of a coin landing on its edge and staying there, it would suggest that the probability of a coin landing on heads is the same as a coin landing on tails. Therefore, the a priori probability of a coin toss landing on heads is equal to a coin toss landing on tails, which is 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. What is posterior probability? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A posterior probability, in Bayesian statistics, is the revised or updated probability of an event occurring after taking into consideration new information. The posterior probability is calculated by updating the prior probability using Bayes' theorem. In statistical terms, the posterior probability is the probability of event A occurring given that event B has occurred.\n",
    "\n",
    "\n",
    "For example, let there be two urns, urn A having 5 black balls and 10 red balls and urn B having 10 black balls and 5 red balls. Now if an urn is selected at random, the probability that urn A is chosen is 0.5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. What is likelihood probability? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you have a probability model with parameters θ.\n",
    "p(x | θ) has two names.\n",
    "It can be called the probability of x (given θ),\n",
    "or the likelihood of θ (given that x was observed).\n",
    "\n",
    "The likelihood is a function of θ. Here are a couple of simple uses:\n",
    "\n",
    "If you observe x and want to estimate the θ that gave rise to it, the maximum-likelihood principle says to choose the maximum-likelihood θ -- in other words, the θ that maximizes p(x | θ).\n",
    "\n",
    "This contrasts with the maximum-a-posteriori or MAP estimate, which is the θ that maximizes p(θ | x). Since x is fixed, this is equivalent to maximizing p(θ) p(x | θ), the product of the prior probability of θ with the likelihood of θ.\n",
    "\n",
    "You can do more with these functions of θ than just maximize them. Much is known about their typical shape as the size of the dataset x increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. What is Naïve Bayes classifier? Why is it named so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifiers are a collection of classification algorithms based on Bayes’ Theorem. It is not a single algorithm but a family of algorithms where all of them share a common principle, i.e. every pair of features being classified is independent of each other.\n",
    "\n",
    "To start with, let us consider a dataset.\n",
    "\n",
    "Consider a fictional dataset that describes the weather conditions for playing a game of golf. Given the weather conditions, each tuple classifies the conditions as fit(“Yes”) or unfit(“No”) for playing golf.\n",
    "\n",
    "The dataset is divided into two parts, namely, feature matrix and the response vector.\n",
    "\n",
    "Feature matrix contains all the vectors(rows) of dataset in which each vector consists of the value of dependent features. In above dataset, features are ‘Outlook’, ‘Temperature’, ‘Humidity’ and ‘Windy’.\n",
    "Response vector contains the value of class variable(prediction or output) for each row of feature matrix. In above dataset, the class variable name is ‘Play golf’.\n",
    "\n",
    "The fundamental Naive Bayes assumption is that each feature makes an:\n",
    "\n",
    "independent\n",
    "equal\n",
    "contribution to the outcome.\n",
    "\n",
    "With relation to our dataset, this concept can be understood as:\n",
    "\n",
    "We assume that no pair of features are dependent. For example, the temperature being ‘Hot’ has nothing to do with the humidity or the outlook being ‘Rainy’ has no effect on the winds. Hence, the features are assumed to be independent.\n",
    "Secondly, each feature is given the same weight(or importance). For example, knowing only temperature and humidity alone can’t predict the outcome accuratey. None of the attributes is irrelevant and assumed to be contributing equally to the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. What is optimal Bayes classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayes Optimal Classifier is a probabilistic model that makes the most probable prediction for a new example.\n",
    "It is described using the Bayes Theorem that provides a principled way for calculating a conditional probability. It is also closely related to the Maximum a Posteriori: a probabilistic framework referred to as MAP that finds the most probable hypothesis for a training dataset.\n",
    "\n",
    "In practice, the Bayes Optimal Classifier is computationally expensive, if not intractable to calculate, and instead, simplifications such as the Gibbs algorithm and Naive Bayes can be used to approximate the outcome.\n",
    "\n",
    "In this post, you will discover Bayes Optimal Classifier for making the most accurate predictions for new instances of data.\n",
    "\n",
    "\n",
    "The Bayes optimal classifier is a probabilistic model that makes the most probable prediction for a new example, given the training dataset.\n",
    "\n",
    "This model is also referred to as the Bayes optimal learner, the Bayes classifier, Bayes optimal decision boundary, or the Bayes optimal discriminant function.\n",
    "\n",
    "Bayes Classifier: Probabilistic model that makes the most probable prediction for new examples.\n",
    "    \n",
    "    The equation below demonstrates how to calculate the conditional probability for a new instance (vi) given the training data (D), given a space of hypotheses (H).\n",
    "\n",
    "P(vj | D) = sum {h in H} P(vj | hi) * P(hi | D)\n",
    "Where vj is a new instance to be classified, H is the set of hypotheses for classifying the instance, hi is a given hypothesis, P(vj | hi) is the posterior probability for vi given hypothesis hi, and P(hi | D) is the posterior probability of the hypothesis hi given the data D.\n",
    "\n",
    "Selecting the outcome with the maximum probability is an example of a Bayes optimal classification.\n",
    "\n",
    "max sum {h in H} P(vj | hi) * P(hi | D)\n",
    "Any model that classifies examples using this equation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Write any two features of Bayesian learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each observed training example can incrementally decrease or increase the estimated\n",
    "probability that a hypothesis is correct.\n",
    "– This provides a more flexible approach to learning than algorithms that completely\n",
    "eliminate a hypothesis if it is found to be inconsistent with any single example.\n",
    "• Prior knowledge can be combined with observed data to determine the final\n",
    "probability of a hypothesis. In Bayesian learning, prior knowledge is provided by\n",
    "asserting\n",
    "– a prior probability for each candidate hypothesis, and\n",
    "– a probability distribution over observed data for each possible hypothesis.\n",
    "• Bayesian methods can accommodate hypotheses that make probabilistic predictions\n",
    "• New instances can be classified by combining the predictions of multiple hypotheses,\n",
    "weighted by their probabilities.\n",
    "• Even in cases where Bayesian methods prove computationally intractable, they can\n",
    "provide a standard of optimal decision making against which other practical methods\n",
    "can be measured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Define the concept of consistent learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". We say that an algorithm L is a consistent learner for\n",
    "a concept class C using hypothesis class H, if for all n, for all c ∈ Cn and for all m, given\n",
    "(x1, c(x1)),(x2, c(x2)), . . . ,(xm, c(xm)) as input, where xi ∈ Xn, L outputs h ∈ Hn such that for\n",
    "i = 1, . . . , m, h(xi) = c(xi). We say that L is an efficient consistent learner if the running time\n",
    "of L is polynomial in n, size(c) and m.\n",
    "A consistent learning algorithm is simply required to output a hypothesis that is consistent\n",
    "with all the training data provided to it. So far, we have not imposed any requirement on\n",
    "the hypothesis class H. This notion of consistency is closely related to the empirical risk\n",
    "minimisation principle in the machine learning literature, where the risk is defined using the\n",
    "zero-one loss.\n",
    "The main result we will prove that if H is “small enough”, something that is made precise\n",
    "in the theorem below, then a consistent learner can be used to derive a PAC-learning algorithm.\n",
    "This theorem shows that short explanatory hypotheses do in fact also possess predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. Write any two strengths of Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is suitable for solving multi-class prediction problems. \n",
    "If its assumption of the independence of features holds true, it can perform better than other models and requires much less training data. \n",
    "Naive Bayes is better suited for categorical input variables than numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. Write any two weaknesses of Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes assumes that all predictors (or features) are independent, rarely happening in real life. This limits the applicability of this algorithm in real-world use cases.\n",
    "This algorithm faces the ‘zero-frequency problem’ where it assigns zero probability to a categorical variable whose category in the test data set wasn’t available in the training dataset. It would be best if you used a smoothing technique to overcome this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "10. Explain how Naïve Bayes classifier is used for\n",
    "\n",
    "1. Text classification\n",
    "\n",
    "2. Spam filtering\n",
    "\n",
    "3. Market sentiment analysis\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 1. Text Classification </b>\n",
    "Text Classification forms a fundamental part of Natural Language Processing. In today’s digital age, we are surrounded by text on our social media accounts, commercials, websites, Ebooks, etc. The majority of this text data is unstructured, so classifying this data can be extremely useful.\n",
    "\n",
    "Step 1\n",
    "Add the required libraries. If not available, use:\n",
    "\n",
    "pip install library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes\n",
    "from sklearn.metrics import accuracy_score\n",
    "Step 2\n",
    "Add the relevant dataset. A user can use the read_csv() method of the pandas’ library to read a CSV file. To do so, use the following command:\n",
    "\n",
    "pd.read_csv(data.csv)\n",
    "Step 3\n",
    "Perform the pre-processing of data, i.e., transforming any raw data into a more understandable NLP context. The following is a list of processes in pre-processing:\n",
    "\n",
    "Remove blank rows in Data, if any\n",
    "Change all the text to lower case\n",
    "Word Tokenization\n",
    "Remove Stop words\n",
    "Remove Non-alpha text\n",
    "Word Lemmatization\n",
    "Step 4\n",
    "Prepare the training and testing dataset using the train_test_split() method of the sklearn library. For better accuracy, keep test_size = 0.25.\n",
    "\n",
    "Step 5\n",
    "Perform encoding on the dataset to differentiate between different labels and assign them a 0 or 1.\n",
    "\n",
    "Encoder = LabelEncoder()\n",
    "Train_Y = Encoder.fit_transform(Train_Y)\n",
    "Test_Y = Encoder.fit_transform(Test_Y)\n",
    "Step 6\n",
    "Convert text data to vectors that the model can understand. The user can make use of TF-IDF`, to reflect on how important a word is to a document in a collection.\n",
    "\n",
    "Step 7\n",
    "Perform machine learning using Naive Bayes classifier:\n",
    "\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf,Train_Y)\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "print(\"Accuracy: \",accuracy_score(predictions_NB, Test_Y)*100)\n",
    "\n",
    "<b> 2. Spam filtering</b>\n",
    "Naive Bayes Mode is one of the two most-used classification models. The basic concept of Naive Bayes Classifier\n",
    "(NBC) is applying Bayes Theorem where the objects or attributes have independence. The detailed process is show\n",
    "below.\n",
    "a) There are two possible classes or categories, denoted by symbol A and A′, to classify each email into in this\n",
    "application: spam and non-spam;\n",
    "b) Vector denoted by ⃑ = < , , , , …  > is used to represent a series of common attributes of spam\n",
    "emails. In this application, the attributes of emails are simply individual words or phrases;\n",
    "c) Every email is represented by a vector denoted by ⃑ = < , , , , …  >, where each  represents the\n",
    "value of the attribute . The email spam filter will first scan through each email searching for those specific words or\n",
    "phrases, and form the vector of each email using the presence or absence of the attributes it detected;\n",
    "d) All attributes take the binary form in this application, meaning that each  is “1” if this word or phrase is present\n",
    "in an email, and “0” if this word or phrase is absent in an email;\n",
    "e) Therefore, the vector representation of each email will look like an “ID number” consisting of a string of “1”s\n",
    "and “0”s, indicating which attributes are present in this particular email;\n",
    "\n",
    "f) The string will be examined by NBC, comparing with the threshold value and resulting the final decision.\n",
    "The mathematic format of NBC (extended version) in the case is shown as follow:\n",
    "⃑ = ⃑ = (⃑ = ⃑|) × ()\n",
    "⃑ = ⃑ × () + (⃑ = ⃑|′) × (′)\n",
    "where ⃑ = ⃑ = ∏ ( = |)\n",
    " .\n",
    "Thus, the final version of the formula used to calculate the probability of an email being spam would be:\n",
    "⃑ = ⃑ = (⃑ = ⃑|) × ()\n",
    "∏ ( = |)\n",
    " × () + ∏ ( = |′)\n",
    " × (′)\n",
    "After computing the probability, some further procedure need to be taken into consideration before making the\n",
    "final decision of classifying the email. Also, the criteria to evaluate the calculated probability and cautiously make\n",
    "NBC’s decision to classify the email in order to minimize the “false positive” cases. One method to approach this\n",
    "issue is to compare the ratio of the probability of this email being spam to the probability of this email being nonspam, and set a threshold value. The equation of this method can be shown as follow, where  the selected threshold\n",
    "value is:\n",
    "(|⃑ = ⃑)\n",
    "(′|⃑ = ⃑)\n",
    ">\n",
    "After formulating, the above inequality becomes:\n",
    "⃑ = ⃑ >\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
